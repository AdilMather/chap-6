{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   6.1 Word-level one-hot encoding (toy example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#               Heading and Coding Examples Sequence\n",
    " \n",
    "    Working with text data\n",
    "        One-hot encoding of words and characters\n",
    "  \n",
    "            6.1 Word-level one-hot encoding (toy example)\n",
    "            6.2 Character-level one-hot encoding (toy example)\n",
    "            6.3 Using Keras for word-level one-hot encoding\n",
    "            6.4 Word-level one-hot encoding with hashing trick (toy example)\n",
    " \n",
    "    Using word embeddings\n",
    "        LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER\n",
    "        USING PRETRAINED WORD EMBEDDINGS\n",
    "     \n",
    "            6.5 Instantiating an Embedding layer\n",
    "            6.6 Loading the IMDB data for use with an Embedding layer\n",
    "            6.7 Using an Embedding layer and classifier on the IMDB data\n",
    "  \n",
    "    Putting it all together: from raw text to word embeddings\n",
    "        DOWNLOADING THE IMDB DATA AS RAW TEXT\n",
    "  \n",
    "            6.8 Processing the labels of the raw IMDB data\n",
    "  \n",
    "    TOKENIZING THE DATA\n",
    "  \n",
    "            6.9 Tokenizing the text of the raw IMDB data\n",
    "  \n",
    "    DOWNLOADING THE GLOVE WORD EMBEDDINGS\n",
    "    PREPROCESSING THE EMBEDDINGS\n",
    "  \n",
    "            6.10 Parsing the GloVe word-embeddings file\n",
    "            6.11 Preparing the GloVe word-embeddings matrix\n",
    "            6.12 Model definition\n",
    " \n",
    "    LOADING THE GLOVE EMBEDDINGS IN THE MODEL\n",
    " \n",
    "            6.13 Loading pretrained word embeddings into the Embedding layer\n",
    "  \n",
    "    TRAINING AND EVALUATING THE MODEL\n",
    "    Compile and train the model.\n",
    "\n",
    "            6.14 Training and evaluation\n",
    "  \n",
    "    Now, plot the model’s performance over time\n",
    "  \n",
    "            6.15 Plotting the results\n",
    "            6.16 Training the same model without pretrained word embeddings\n",
    "            6.17 Tokenizing the data of the test set\n",
    "   \n",
    "    Next, load and evaluate the first model.\n",
    "   \n",
    "            6.18 Evaluating the model on the test set\n",
    "   \n",
    "    Wrapping up\n",
    "   \n",
    "    Understanding recurrent neural networks\n",
    "   \n",
    "            6.19 Pseudocode RNN\n",
    "            6.20 More detailed pseudocode for the RNN\n",
    "            6.21 Numpy implementation of a simple RNN\n",
    "    \n",
    "    A recurrent layer in Keras\n",
    "        The following example returns the full state sequence:\n",
    "        \n",
    "        \n",
    "            6.22 Preparing the IMDB data\n",
    "     \n",
    "    Let’s train a simple recurrent network using an Embedding layer and a SimpleRNN ayer.\n",
    "     \n",
    "            6.23 Training the model with Embedding and SimpleRNN layers\n",
    "      \n",
    "    Now, let’s display the training and validation loss and accuracy\n",
    "      \n",
    "            6.24 Plotting results\n",
    "       \n",
    "    Understanding the LSTM and GRU layers\n",
    "        \n",
    "            6.25 Pseudocode details of the LSTM architecture (1/2)\n",
    "            6.26 Pseudocode details of the LSTM architecture (2/2)\n",
    "        \n",
    "    A concrete LSTM example in Keras\n",
    "        \n",
    "            6.27 Using the LSTM layer in Keras\n",
    " \n",
    "    Wrapping up\n",
    "     \n",
    "    Advanced use of recurrent neural networks\n",
    "        A temperature-forecasting problem\n",
    "         \n",
    "            6.28 Inspecting the data of the Jena weather dataset\n",
    " \n",
    "    Now, convert all 420,551 lines of data into a Numpy array\n",
    " \n",
    "            6.29 Parsing the data\n",
    "            6.30 Plotting the temperature timeseries\n",
    "            6.31 Plotting the first 10 days of the temperature timeseries\n",
    " \n",
    "    Preparing the data\n",
    " \n",
    "            6.32 Normalizing the data\n",
    "            6.33 Generator yielding timeseries samples and their targets\n",
    "            6.34 Preparing the training, validation, and test generators\n",
    " \n",
    "    A common-sense, non-machine-learning baseline\n",
    "        Here’s the evaluation loop.\n",
    "         \n",
    "            6.35 Computing the common-sense baseline MAE\n",
    "            6.36 Converting the MAE back to a Celsius error\n",
    "  \n",
    "    A basic machine-learning approach\n",
    "      \n",
    "            6.37 Training and evaluating a densely connected model\n",
    "   \n",
    "    Let’s display the loss curves for validation and training\n",
    "       \n",
    "            6.38 Plotting result\n",
    "\n",
    "    A first recurrent baseline\n",
    "    \n",
    "            6.39 Training and evaluating a GRU-based model\n",
    "\n",
    "    Using recurrent dropout to fight overfitting\n",
    "     \n",
    "            6.40 Training and evaluating a dropout-regularized GRU-based model\n",
    "\n",
    "    Stacking recurrent layers\n",
    "    \n",
    "            6.41 Training and evaluating a dropout-regularized, stacked GRU model\n",
    "\n",
    "    Using bidirectional RNNs\n",
    "     \n",
    "            6.42 Training and evaluating an LSTM using reversed sequences\n",
    "            6.43 Training and evaluating a bidirectional LSTM\n",
    "            6.44 Training a bidirectional GRU\n",
    " \n",
    "    Going even further\n",
    "    Wrapping up\n",
    "    Markets and machine learning\n",
    " \n",
    "    Sequence processing with convnets\n",
    "        Understanding 1D convolution for sequence data\n",
    "        1D pooling for sequence data\n",
    "        Implementing a 1D convnet\n",
    " \n",
    "            6.45 Preparing the IMDB data\n",
    " \n",
    "    This is the example 1D convnet for the IMDB dataset\n",
    "     \n",
    "            6.46 Training and evaluating a simple 1D convnet on the IMDB data\n",
    "  \n",
    "    Combining CNNs and RNNs to process long sequences\n",
    "      \n",
    "            6.47 Training and evaluating a simple 1D convnet on the Jena data\n",
    "            6.48 Preparing higher-resolution data generators for the Jena dataset\n",
    "\n",
    "    This is the model, starting with two Conv1D layers and following up with a GRU layer.\n",
    "    \n",
    "            6.49 Model combining a 1D convolutional base and a GRU layer\n",
    "    \n",
    "    Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Deep Learning For Text and Sequences ( NLP )\n",
    "\n",
    "*Note : Two fundamentals of Deep Learninng algorithms for sequence processing\n",
    "        \n",
    "        1. RNN Recurrent Neural Network\n",
    "        2. 1D Convnets ( One dimensional version of 2D convnets covered in previous chapters ) \n",
    "                       ( 1D convents for sequence processing )\n",
    "                       \n",
    "This chapter covers : Sequences of words\n",
    "                      Sequences of characters\n",
    "                      With both ways : One-hot-encoding\n",
    "                                       Embaded coding\n",
    "                      Vectorization\n",
    "                      Tokenization\n",
    "                                       \n",
    "Applications of these algorithms include the following:\n",
    "\n",
    " 1. Document classification and timeseries classification, such as identifying the\n",
    "    topic of an article or the author of a book\n",
    "    \n",
    " 2. Timeseries comparisons, such as estimating how closely related two documents or two stock tickers are\n",
    "\n",
    " 3. Sequence-to-sequence learning, such as decoding an English sentence into French\n",
    " \n",
    " 4. Sentiment analysis, such as classifying the sentiment of tweets or movie reviews\n",
    "    as positive or negative\n",
    "    \n",
    " 5. Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data\n",
    "\n",
    "Working with text data :\n",
    "\n",
    " 1. Text is the form of sequence data\n",
    " \n",
    " 2. Eiher a sequence of characters or a sequence of words\n",
    " \n",
    " 3. Bt it’s most common to work at the level of words\n",
    " \n",
    " 4. It is sufficient for applications including document classification, sentimentanalysis, author identification, \n",
    "    and even question-answering (QA) (in a constrained context)\n",
    "    \n",
    " 5. Deep learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs\n",
    " \n",
    " 6. the same way that computer ision is pattern recognition applied to pixels \n",
    " \n",
    "  # VECTORIZATION\n",
    "  \n",
    " 7. Like ll other neural networks, deep-learning models don’t take as input raw text they only work with\n",
    "    numeric tensors ( Binary ) by VECTORIZATION \n",
    "    \n",
    " 8. Vectorizing text is the process of transforming text into numeric tensors. This can be done in multiple ways: \n",
    "         \n",
    "         a. Segment text into words, and transform each word into a vector.\n",
    "         \n",
    "         b. Segment text into characters, and transform each character into a vector.\n",
    "         \n",
    "         c. Extract n-grams of words or characters, and transform each n-gram into a vector. N-grams are\n",
    "            overlapping groups of multiple consecutive words or characters\n",
    "\n",
    "#   TOKENIZATION \n",
    "\n",
    " 9. the different units into which you can break down text (words, characters, or n-grams) are called tokens,\n",
    "    and breaking text into such tokens is called tokenization\n",
    "    \n",
    " 10. All text-vectorization processes consist of applying some tokenization scheme and then associating\n",
    "     numeric vectors with the generated tokens\n",
    "     \n",
    " 11. These vectors, packed into sequence tensors, are fed into deep neural networks\n",
    " \n",
    "#    ONE-HOT-ENCODING and TOKEN EMBEDDING (WORD EMBEDDING)\n",
    "\n",
    " 12. There are multiple ways to associate a vector with a token. In this section, I’ll present two major ones:\n",
    "     one-hot encoding of tokens and \n",
    "     token embedding (typically used exclusively for words, and called word embedding)\n",
    "     \n",
    " 13. The remainder of this section explains these techniques and shows how to use them to go from raw text\n",
    "     to a Numpy tensor that you can send to a Keras network.  \n",
    "\n",
    "                                            Text\n",
    "                                “The cat sat on the mat.”\n",
    "\n",
    "                                            Tokens\n",
    "                        “the”, “cat”, “sat”, “on”, “the”, “mat”, “.”\n",
    "\n",
    "                                    Vector encoding of the tokens\n",
    "                                     0.0 0.0 0.4 0.0 0.0 1.0 0.0\n",
    "                                     0.5 1.0 0.5 0.2 0.5 0.5 0.0\n",
    "                                     1.0 0.2 1.0 1.0 1.0 0.0 0.0\n",
    "                                     the cat sat on the mat .                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                   One-hot-encoding of words and characters\n",
    "\n",
    " 1. One-hot encoding is the most common, most basic way to turn a token into a vectorOne-hot encoding\n",
    "    is the most common, most basic way to turn a token into a vector\n",
    "   \n",
    " 2. Done it in action in the initial IMDB and Reuters examples in chapter 3\n",
    "    (done with words, in that case) \n",
    "   \n",
    " 3. It consists of associating a unique integer index with every word and then turning this integer\n",
    "    index i into a binary vector of size N (the size of the vocabulary)\n",
    "   \n",
    " 4. the vector is all zeros except for the ith entry, which is 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#               one-hot-encoding  ( Word Level )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial data: one entry per sample, in this example, a sample is a sentence,\n",
    "but it could be an entire document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builds an index of all tokens in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizes the samples via the split method. In real life, you’d also strip\n",
    "punctuation and special characters from the samples.\n",
    "\n",
    "Assigns a unique index to each unique word. Note that you don’t attribute index 0 to anything.\n",
    "                token_index[word] = len(token_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizes the samples. You’ll only consider the first max_length words in each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where you store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros(shape=(len(samples),\n",
    "                            max_length,\n",
    "                            max(token_index.values()) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enumerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
